{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00315902-b10a-43b6-a79b-4e45ec5d1442",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9c33b-5a50-48ff-a4be-24300f6fef8e",
   "metadata": {},
   "source": [
    "Text preprocessing is an essential step in natural language processing (NLP) tasks. It involves transforming raw text data into a format that is more suitable for analysis and machine learning algorithms. In this tutorial, we will cover various common techniques for text preprocessing. Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172c0e6-59cc-4a2a-9a27-dc5cc6d3bafd",
   "metadata": {},
   "source": [
    "### 1. Lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675a1b6-c666-46de-ae58-576ecb70e78b",
   "metadata": {},
   "source": [
    "Converting all text to lowercase can help to normalize the data and reduce the vocabulary size. It ensures that words in different cases are treated as the same word. For example, \"apple\" and \"Apple\" will both be transformed to \"apple\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2ec14d8-766c-4b21-b876-c467ced7f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello, I am your AI companion R2D2.\"\n",
    "lower_sent = sentence.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97db8089-823b-4048-8317-05eb7403a042",
   "metadata": {},
   "source": [
    "### 2. Removal of Punctuation and Special Character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486d45b-10d0-4491-813f-156859c07523",
   "metadata": {},
   "source": [
    "Punctuation marks and special characters often do not add much meaning to the text and can be safely removed. Common punctuation marks include periods, commas, question marks, and exclamation marks. You can use regular expressions or string operations to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a9a9281-57e7-4b2e-a202-78948a1e407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_punctuation = ['.', ',', ':', ';', '!', '?', '(', ')', '\"', \"'\"]\n",
    "result = \"\"\n",
    "for each in lower_sent:\n",
    "    if each not in common_punctuation:\n",
    "        result += each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b83aac4f-a6d7-49cd-8d61-919cb7663d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello i am your ai companion r2d2\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "cleaned = re.sub(r'[^\\w\\s]', '', lower_sent)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbba6c1-5f50-44cb-99ca-00ca8462a9d7",
   "metadata": {},
   "source": [
    "### 3. StopWord Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c13779-f5be-4f2a-ad38-9efbb0cde281",
   "metadata": {},
   "source": [
    "Stop words are commonly occurring words in a language, such as \"a,\" \"an,\" \"the,\" \"is,\" and \"in.\" These words provide little semantic value and can be removed to reduce noise in the data. Libraries like NLTK provide a list of predefined stop words for different languages.\n",
    "\n",
    "Before using the code make sure you downloaded all the stopwords uning the first shell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7ea153a-564d-4406-80d5-8d2cd113c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2143c641-1d0e-482c-88e1-3ba5d11def3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65d90ca8-6b54-4f9e-a5cf-8ca6e6a53e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9c99c00-d4b7-4dcf-9772-a1ff569ed4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ea71eaf-cc83-458f-b0c1-fb8248084506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello ai companion r2d2\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "filtered = [word for word in cleaned.split() if word not in stopwords]\n",
    "filtered = \" \".join(filtered)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c8965-097c-47d6-83f9-e571a9de43eb",
   "metadata": {},
   "source": [
    "### 4. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f73bbba-b4e9-4b79-99d3-f55ceaa1eb21",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down a piece of text into smaller units called tokens. These tokens can be words, subwords, or even characters, depending on the level of granularity desired. Tokenization is a fundamental step in text preprocessing and is crucial for various natural language processing (NLP) tasks, such as machine translation, sentiment analysis, and language generation.\n",
    "\n",
    "Here's a detailed explanation of tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce67c2-4af0-4094-973a-7b0b28e10d03",
   "metadata": {},
   "source": [
    "#### a. Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e34261-95b1-471b-a1bc-6358eb5b61c1",
   "metadata": {},
   "source": [
    "Word tokenization is the most common form of tokenization, where the text is split into individual words. For example, given the sentence \"Tokenization is important for NLP tasks,\" the word tokens would be: [\"Tokenization\", \"is\", \"important\", \"for\", \"NLP\", \"tasks\"].\n",
    "\n",
    "Word tokenization is typically performed using whitespace as the delimiter. However, it's important to handle cases like punctuation marks, contractions, and hyphenated words correctly. For example, \"don't\" should be tokenized as [\"do\", \"n't\"] instead of [\"don\", \"'\", \"t\"].\n",
    "\n",
    "Libraries like NLTK, spaCy, and the tokenizers package provide ready-to-use word tokenization functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0048c-6941-4845-81f4-a008bb47edb4",
   "metadata": {},
   "source": [
    "Before running any of these tokenization techniques, make sure you have `punkt` downloaded. `punkt` refers to the Punkt Tokenizer, which is a pre-trained unsupervised machine learning model for sentence tokenization. The NLTK Punkt Tokenizer is trained on large corpora and is capable of handling a wide range of sentence boundary detection for multiple languages. It uses a combination of rule-based heuristics and statistical models to identify sentence boundaries accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ca38da1-795e-4754-b5e5-3eb83c27981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7aa2dad5-c17a-44e8-990f-524d074f3c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'ai', 'companion', 'r2d2']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(filtered)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9170d-8464-4468-8e49-874550242888",
   "metadata": {},
   "source": [
    "### 5. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b3329-8708-4c41-888d-c50584337821",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are techniques used in natural language processing (NLP) to reduce words to their base or root forms. Both approaches aim to normalize words and reduce inflectional variations, enabling better analysis and comparison of words. However, they differ in their methods and outputs. Let's dive into each technique in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8f449-4976-4421-8aaa-61469c5508fa",
   "metadata": {},
   "source": [
    "#### a. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20355236-9d82-4ccf-aee8-812266ec252b",
   "metadata": {},
   "source": [
    "Stemming is a process of reducing words to their base or root forms by removing prefixes or suffixes. The resulting form is often a stem, which may not be an actual word itself. The primary goal of stemming is to simplify the vocabulary and group together words with the same base meaning.\n",
    "\n",
    "For example, when using a stemming algorithm on the words \"running,\" \"runs,\" and \"ran,\" the common stem would be \"run.\" The stemming process cuts off the suffixes (\"-ning,\" \"-s,\" and \"-\"), leaving behind the core form of the word.\n",
    "\n",
    "Stemming algorithms follow simple rules and heuristics based on linguistic patterns, rather than considering the context or part of speech of the word. Some popular stemming algorithms include the Porter stemming algorithm, the Snowball stemmer (which supports multiple languages), and the Lancaster stemming algorithm.\n",
    "\n",
    "Stemming is a computationally lightweight approach and can be useful in certain cases where the exact word form is not crucial. However, it may produce stems that are not actual words, leading to potential loss of meaning and ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "959caaff-f094-4342-8885-73188e6fdfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"running\", \"runs\", \"runner\", \"better\", \"good\", \"best\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79072ec7-5371-4068-a1bb-e97fd90684a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['run', 'run', 'runner', 'better', 'good', 'best']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_word = [stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed words:\", stemmed_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8ba97-4ac7-411a-9f56-bafe5efe19f8",
   "metadata": {},
   "source": [
    "#### b. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521e310-5932-4f8a-b56e-342816c7e48d",
   "metadata": {},
   "source": [
    "Lemmatization, on the other hand, aims to reduce words to their canonical or dictionary forms, known as lemmas. Unlike stemming, lemmatization considers the context and part of speech (POS) of the word to generate meaningful lemmas. The resulting lemmas are actual words found in the language's dictionary.\n",
    "\n",
    "For example, when lemmatizing the words \"running,\" \"runs,\" and \"ran,\" the lemma for each would be \"run.\" Lemmatization takes into account the POS information to accurately determine the base form of the word.\n",
    "\n",
    "Lemmatization algorithms use linguistic rules and morphological analysis to identify the appropriate lemma. They often rely on language-specific resources, such as word lists and morphological databases. Some popular lemmatization tools include the WordNet lemmatizer and the spaCy library (which supports lemmatization for multiple languages).\n",
    "\n",
    "Lemmatization typically produces more accurate and meaningful results compared to stemming because it retains the core meaning of words. It is especially useful in tasks that require precise word analysis, such as information retrieval, question answering, and sentiment analysis.\n",
    "\n",
    "However, lemmatization can be more computationally intensive compared to stemming due to its reliance on POS tagging and language-specific resources.\n",
    "\n",
    "Before running any of these tokenization techniques, make sure you have `wordnet` downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a4bfac4-6b65-4f1a-92a0-8d88c256d48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['run', 'run', 'runner', 'better', 'good', 'best']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatize_words = [lemmatizer.lemmatize(word, wordnet.VERB) for word in words]\n",
    "print(\"Lemmatized words:\", lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a15d7365-7fb6-4daa-9bd7-b4181b9fb62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tasty'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('tasty')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d3b14-63e9-4884-9989-7e896a4c9ca9",
   "metadata": {},
   "source": [
    "When deciding between stemming and lemmatization, consider the trade-off between simplicity and accuracy. If you require speed and a broad reduction of word forms, stemming may be sufficient. However, if you need more accurate analysis and want to preserve the semantic meaning of words, lemmatization is generally the preferred choice.\n",
    "\n",
    "It's important to note that both stemming and lemmatization have limitations. They may not always produce the correct base forms, especially for irregular words or those not present in the chosen language's dictionary. Contextual information, such as word sense disambiguation, can further enhance the accuracy of both techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5ebe8-7178-431d-963b-9cfeb3a7fa60",
   "metadata": {},
   "source": [
    "### 6. Convert Word to number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b59dd7-cdd2-4f7f-8c73-f3934aeda2fd",
   "metadata": {},
   "source": [
    "To convert words into numbers, you can use various techniques such as word embedding, one-hot encoding, or bag-of-words representation. Each technique has its own advantages and is used based on the specific requirements of the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e16602-4ca4-4400-b2f3-1163886c0ebf",
   "metadata": {},
   "source": [
    "#### 1. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c504d-38c6-4528-a703-a0127f4314fe",
   "metadata": {},
   "source": [
    "Word embedding is a technique used in natural language processing (NLP) to represent words as dense vectors of real numbers. These vectors capture semantic and syntactic relationships between words based on their contextual usage in text data.\n",
    "\n",
    "The key idea behind word embedding is to map words from a high-dimensional space (the vocabulary space) to a lower-dimensional space (the embedding space) where similar words are closer together in the vector space. This allows machine learning models to better understand the meaning of words and their relationships, which can improve the performance of various NLP tasks such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "Popular word embedding models include **Word2Vec**, **GloVe (Global Vectors for Word Representation)**, and **fastText**. These models are trained on large text corpora to learn the relationships between words and generate meaningful word embeddings that capture semantic and syntactic properties of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c72e6d5-4480-42fb-8ad1-321fb14fd509",
   "metadata": {},
   "source": [
    "##### a. Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f11a4-dce3-4215-9b76-7300c0f7273f",
   "metadata": {},
   "source": [
    "\n",
    "Bag of Words (BoW) is a common technique used in natural language processing (NLP) for text analysis and document classification. It is a simple and flexible way of extracting features from text data for use in machine learning models.\n",
    "\n",
    "The basic idea behind the Bag of Words model is to represent text data as a multiset (or \"bag\") of words, disregarding grammar and word order. It involves the following steps:\n",
    "\n",
    "- **Tokenization**: The text is split into individual words or tokens.\n",
    "- **Vocabulary Creation**: A vocabulary of unique words in the entire dataset is created.\n",
    "- **Vectorization**: Each document (or text sample) is represented as a vector, where each element of the vector corresponds to a word in the vocabulary. The value of each element is the frequency of the corresponding word in the document.\n",
    "\n",
    "**Bag of Words** model is the simplest and most popular form of word embedding. The key idea of **BoW** models is to encode every word in the vocabulary as one-hot-encoded vector.\n",
    "\n",
    "If r1, r2 and r3 be three records, the vectors corresponding to r1, r2 and r3 be v1, v2 and v3 respectively such that r1 and r2 are more similar to each other as compared to r3. Then, as general understanding, the vector distance between v1 and v2 is less than that between v1 and v3 or v2 and v3.\n",
    "\n",
    "<p align=\"center\"><b>\n",
    "    distance (v1, v2) < distance (v1, v3)<br/>\n",
    "    similarity (r1, r2) > similarity (r1, r3)\n",
    "</b></p>\n",
    "\n",
    "For easy understanding, let us consider a sweet example. Let there be three reviews for a product in ecommerce site as:\n",
    "\n",
    "    r1: This product is good and is affordable.\n",
    "    r2: This product is not good and affordable.\n",
    "    r3: This product is good and cheap.\n",
    "\n",
    "Let's see how BoW encodes the text data to machine compatible form. Follow along with the below points:\n",
    "\n",
    "**I. Construct a set of all the unique words present in the corpus:**\n",
    "\n",
    "    { this, product, is, good, and, affordable, not, cheap }\n",
    "\n",
    "There are a total of 8 uique words in the set formed. So the size of the vector generated for each review will be 8 as well, with the index position starting from 0 and ending to 7 i.e. \n",
    "\n",
    "    { 0: this, 1: product, 2: is, 3: good, 4: and, 5: affordable, 6: not, 7: cheap }\n",
    "\n",
    "**II. Construct a d-dimensional vector for each review separately:**\n",
    "\n",
    "Construct a d-dimensional vector (*d* being the vocabulary size) for each review. Each index/dimension of the vector corresponds to a unique word in the vocabulary. The value in each cell of the vector represents the number of times the word with that index occurs in the corpus.\n",
    "\n",
    " d | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
    "---|---|---|---|---|---|---|---|---|\n",
    "**v1**| 1 | 1 | 2 | 1 | 1 | 1 | 0 | 0 |\n",
    "**v2**| 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 |\n",
    "**v3**| 1 | 1 | 1 | 1 | 1 | 0 | 0 | 1 |\n",
    "\n",
    "<p style=\"text-align:center;\"><i><b>Table :</b> 8 dimensional vector representation of each review</i></p>\n",
    "\n",
    "#### Objective\n",
    "\n",
    "Similar texts (reviews, in this case) must result closer vector.\n",
    "\n",
    "    distance(v1-v2) = √((1-1)²+(1-1)²+(2-1)²+(1-1)²+(1-1)²+(1-1)²+(0-1)²+(0-0)²) = √2\n",
    "    distance(v1-v3) = √((1-1)²+(1-1)²+(2-1)²+(1-1)²+(1-1)²+(1-0)²+(0-0)²+(0-1)²) = √3 \n",
    "\n",
    "The Euclidean distance between vectors v1 and v2 is less than that between v1 and v3. However the meaning of review r1 is completely opposite to that of review r2. Thus, BoW does not preserve the semantic meaning of a words and fails to work when there is small change in the text statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "341869d6-99d4-4427-88b0-939b2918eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text  import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "785847ef-1eb8-4f86-91fa-cbb50519813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This product is good and is affordable.\",\n",
    "    \"This product is not good and affordable.\",\n",
    "    \"This product is good and cheap.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "output = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a50618a0-2a3e-4dc6-b043-213f314b96d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd479c13-3705-4f3d-ba9b-3ac5d3faa7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "310332b1-6f82-49db-8b53-cc2b1ecb6c14",
   "metadata": {},
   "source": [
    "##### b. TF-IDF: Term Frequency- Inverse Document Frequency\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "- Vector length is insanely large for large corpus.\n",
    "- BoW results to sparse matrix, which is what we would like to avoid.\n",
    "- Retains no information about grammar and ordering of words in a corpus.\n",
    "\n",
    "##### TF-IDF\n",
    "\n",
    "In NLP an independent text entity is known as document and the collection of all these documents over the project space is known as corpus. *tf-idf* stands for Term Frequency-Inverse Document Frequency. The entire technique can be studied by studying *tf* and *idf* separately.\n",
    "\n",
    "**Term-Frequency** is a measure of frequency of appearance of term *t* in a document *d*. In other words, the probability of finding term *t* in a document *d*. \n",
    "\n",
    "<p align=\"center\">\n",
    "    {% mathjax %}\n",
    "        tf_{t,d} = \\frac{No \\hspace{1mm} of \\hspace{1mm} times \\hspace{1mm} t \\hspace{1mm} appears \\hspace{1mm} in \\hspace{1mm} d}{Total \\hspace{1mm} no \\hspace{1mm} of \\hspace{1mm} terms \\hspace{1mm} in \\hspace{1mm} d}\n",
    "    {% endmathjax %}\n",
    "</p>\n",
    "\n",
    "**Inverse-Document-Frequency** is a measure of inverse of probability of finding a document that contains term t in a corpus. In other words, a measure of the importance of term t.\n",
    "\n",
    "<p align=\"center\">\n",
    "    {% mathjax %}\n",
    "        idf_{t} = log \\hspace{1mm} \\frac{Total \\hspace{1mm} no \\hspace{1mm} of \\hspace{1mm} documents \\hspace{1mm} in \\hspace{1mm} corpus}{No \\hspace{1mm} of \\hspace{1mm} documents \\hspace{1mm} with \\hspace{1mm} term \\hspace{1mm} t}\n",
    "    {% endmathjax %}\n",
    "</p>\n",
    "\n",
    "We can now compute the *tf-idf* score for each word in the corpus. *tf-idf* gives us the similarity between two documents in the corpus. Words with a higher score are more important. *tf-idf* score is high when both *idf* and *tf* values are high. So, *tf-idf* gives more importance to words that are:\n",
    "\n",
    "- More frequent in the entire corpus\n",
    "- Rare in the corpus but frequent in the document.\n",
    "\n",
    "Now this *tf-idf* score is used as a value for each cell of the document-term matrix, just like the frequency of words in case of Bag-of-Words. The formula below is used to compute *tf-idf* score for each cell:\n",
    "\n",
    "<p align=\"center\">\n",
    "    {% mathjax %}\n",
    "        (tf-idf)_{t,d} = tf_{t,d} * idf_{t}\n",
    "    {% endmathjax %}\n",
    "</p>\n",
    "\n",
    "While computing *tf*, all terms are considered equally important. However, it is known that certain terms, such as *is*, *of*, *and*, *that*, *the*, etc may appear a lot of times but have no or little importance. Thus we need to weigh down such frequent terms while scaling the rare ones up using *idf*.\n",
    "\n",
    " Term | tf (r1) | tf (r2) | tf (r3)| idf | tf-idf (r1) | tf-idf (r2) | tf-idf (r3)\n",
    "---|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "this| 1/7 | 1/7 | 1/7 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "product| 1/7 | 1/7 | 1/7 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "is| 2/7 | 1/7 | 1/7 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "good| 1/7 | 1/7 | 1/7 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "and| 1/7 | 1/7 | 1/7 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "affordable| 1/7 | 1/7 | 0 | 0.176 | 0.025 | 0.025 | 0.000 |\n",
    "not| 0 | 1/7 | 0 | 0.477 | 0.000 | 0.068 | 0.000 |\n",
    "cheap| 0 | 0 | 1/7 | 0.477 | 0.000 | 0.000 | 0.068 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1448a1-b009-45c8-889c-0399735434f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "336f237d-e912-4e10-acb7-4c7da7d14bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "363a9cbc-c4a7-42df-aa39-3cf2f649abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This product is good and is affordable.\",\n",
    "    \"This product is not good and affordable.\",\n",
    "    \"This product is good and cheap.\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "output = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a72b461-3588-4175-bb03-8db57ebc032d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41434513, 0.32177595, 0.        , 0.32177595, 0.64355191,\n",
       "        0.        , 0.32177595, 0.32177595],\n",
       "       [0.4172334 , 0.32401895, 0.        , 0.32401895, 0.32401895,\n",
       "        0.54861178, 0.32401895, 0.32401895],\n",
       "       [0.        , 0.35653519, 0.60366655, 0.35653519, 0.35653519,\n",
       "        0.        , 0.35653519, 0.35653519]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7a6a0-3992-4faf-9dcf-775cf2fe66b0",
   "metadata": {},
   "source": [
    "## Popular WordEmbedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc75fa-f3c3-44c3-a21c-de765c6ccbd2",
   "metadata": {},
   "source": [
    "### 1. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe70ef8-afa1-4aae-815e-37dada67bc7b",
   "metadata": {},
   "source": [
    "Word2Vec is a popular technique in natural language processing (NLP) used to create dense vector representations of words, also known as word embeddings. These word embeddings capture semantic relationships between words based on their contextual usage in text data.\n",
    "\n",
    "Word2Vec is based on the idea that words that occur in similar contexts tend to have similar meanings. The model is trained on a large corpus of text data and learns to predict the context (surrounding words) of a target word within a sentence. There are two main architectures used in Word2Vec:\n",
    "\n",
    "Word2Vec is a popular technique for learning word embeddings, which are dense vector representations of words in a continuous vector space. Word embeddings capture semantic relationships between words, allowing machines to understand and work with words in a more meaningful way. Word2Vec was introduced by researchers at Google in 2013, and it has since become one of the foundational techniques in natural language processing (NLP) and other related fields.\n",
    "\n",
    "The basic idea behind Word2Vec is to represent each word in a high-dimensional vector space, where words with similar meanings or contexts are located close to each other. The key intuition behind Word2Vec is the distributional hypothesis, which posits that words appearing in similar contexts tend to have similar meanings. For example, in the sentences \"I love cats\" and \"I adore felines,\" the words \"love\" and \"adore\" are likely to be used in similar contexts and have similar semantic meanings.\n",
    "\n",
    "Word2Vec can be trained using two main architectures: Continuous Bag of Words (CBOW) and Skip-gram. Let's explore each of these in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badc245-4859-48f1-aad4-b2f660ceb766",
   "metadata": {},
   "source": [
    "#### a. Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7504cc-1ca8-4d57-a9a9-2d04475c075b",
   "metadata": {},
   "source": [
    "CBOW aims to predict a target word based on its surrounding context words. Given a sequence of words in a sentence, CBOW tries to predict the middle word based on the surrounding context words. The context window size determines how many words before and after the target word are considered as the context.\n",
    "\n",
    "For example, consider the sentence: \"The cat sat on the mat.\" If we set the context window size to 2 and assume \"sat\" is the target word, CBOW will use the context words \"The,\" \"cat,\" \"on,\" and \"the\" to predict the word \"sat.\"\n",
    "\n",
    "The architecture involves the following steps:\n",
    "- Convert the context words to their corresponding word embeddings.\n",
    "- Average these embeddings to create a context vector.\n",
    "- Use this context vector as input to a neural network to predict the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7445fdac-c53f-4e67-83b9-87cf945197d6",
   "metadata": {},
   "source": [
    "#### b. Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6c8f2a-133d-42a2-830d-ef50cfdd5345",
   "metadata": {},
   "source": [
    "Skip-gram works in the opposite way of CBOW. It aims to predict context words given a target word. In other words, it tries to find the context words that are most likely to appear in the given sentence with a particular target word.\n",
    "\n",
    "For the same example sentence, \"The cat sat on the mat,\" if \"sat\" is the target word, Skip-gram will try to predict the context words \"The,\" \"cat,\" \"on,\" and \"the.\"\n",
    "\n",
    "The architecture involves the following steps:\n",
    "- Convert the target word to its corresponding word embedding.\n",
    "- Use this embedding as input to a neural network to predict the context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295a1bd-0b58-4561-b32d-5aa0171f493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "# Load the Brown corpus\n",
    "sentences = brown.sents()\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, sg=0)\n",
    "\n",
    "# Get the vector representation of a word\n",
    "vector = model.wv['word']\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fce2a7-9265-4ae5-b824-9bbda7350547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635292c9-16f8-4078-ac4b-e6de64834fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus (list of sentences)\n",
    "corpus = [\n",
    "    \"I love cats\",\n",
    "    \"I adore felines\",\n",
    "    \"Dogs are loyal\",\n",
    "    \"Cats and dogs are pets\",\n",
    "    \"The sun is shining\"\n",
    "]\n",
    "\n",
    "tokenized = [sentence.lower().split() for sentence in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f317fdac-b63e-4dca-94b4-40b389338657",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = Word2Vec(sentences=tokenized_, vector_size=100, window=2, sg=0, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505264d-c574-400b-8c84-60333941c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model.wv.most_similar(positive=['cats'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c052a2e-9085-4665-96f2-4c75df72f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_model = Word2Vec(sentences=tokenized_, vector_size=100, window=2, sg=1, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7999e12-3042-4765-b53a-112f95847a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_model.wv.most_similar(positive=['cats'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5943e-fbeb-4195-b786-ac3589273715",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_model.wv.get_mean_vector(['cat','sun','loyal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca955b43-1197-4b94-93e7-39055f84bde8",
   "metadata": {},
   "source": [
    "### 2. BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c8daa-5074-4f9f-9a9e-e0d52f5db61c",
   "metadata": {},
   "source": [
    "BERT stands for Bidirectional Representation for Transformers. It was proposed by researchers at Google Research in 2018. Although the main aim of that was to improve the understanding of the meaning of queries related to Google Search. A study shows that Google encountered 15% of new queries every day. Therefore, it requires the Google search engine to have a much better understanding of the language in order to comprehend the search query.\n",
    "\n",
    "To improve the language understanding of the model. BERT is trained and tested for different tasks on a different architecture. Some of these tasks with the architecture discussed below.\n",
    "\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a neural network model that was pre-trained on a massive dataset of text and code. It can be used for a variety of natural language processing (NLP) tasks, such as question answering, text classification, and sentiment analysis.\n",
    "\n",
    "**How does BERT work?**\n",
    "\n",
    "BERT is a transformer-based model, which means that it uses a stack of self-attention layers to learn the relationships between words in a sentence. The model is pre-trained on a massive dataset of text and code, which allows it to learn the contextual meaning of words.\n",
    "\n",
    "**How to use BERT for embedding?**\n",
    "\n",
    "BERT can be used to generate word embeddings, which are vector representations of words that capture their semantic meaning. To generate word embeddings using BERT, you first need to tokenize the input text into individual words or subwords (using the BERT tokenizer). You can then pass the tokenized input through the BERT model to generate a sequence of hidden states. The hidden states can then be used to represent the words in the input text.\n",
    "\n",
    "To implement BERT, we will use HuggingFace's `transformers` library and `transformers` requires `pytorch` installed. So let's begin by installing the required libraries.\n",
    "\n",
    "```\n",
    "pip3 install torch\n",
    "pip3 install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d7283-ac06-4e00-8c7a-4199d7936350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genism.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134639a4-0c63-41d6-a5f8-ec6f0294231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(genism.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9947a-1546-40d8-aeef-04037b91f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.dowloader.info()['models'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce803a-7cfb-4657-9f0b-771185893983",
   "metadata": {},
   "source": [
    "<Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208737f3-ff60-4e9c-8e61-ef3b861b4171",
   "metadata": {},
   "source": [
    "### What steps to do after text preprocessing for Supervised Machine Learning problens using NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567fe9f4-cd5f-40da-bbc0-bc73af78ff90",
   "metadata": {},
   "source": [
    "After text preprocessing for a supervised machine learning problem using NLP, you typically need to follow these steps:\n",
    "1. Text Vectorization:\n",
    "    - Bag of Words (BoW): Convert text data into vectors using methods like Count Vectorizer or TF-IDF Vectorizer.\n",
    "    - Word Embeddings: Use word embeddings such as Word2Vec, GloVe, or FastText.\n",
    "    - Sentence Embeddings: Use models like Sentence-BERT to convert sentences into dense vectors.\n",
    "\n",
    "2. Feature Selection/Engineering (if necessary):\n",
    "    - Select the most important features that contribute to the model’s performance.\n",
    "    - Create additional features if necessary (e.g., length of text, number of stop words).\n",
    "\n",
    "3. Splitting Data:\n",
    "    - Split your dataset into training and testing sets, typically using a function like `train_test_split` from `sklearn.model_selection`.\n",
    "\n",
    "4. Model Training:\n",
    "    - Choose an appropriate machine learning model based on your problem (e.g., classification, regression).\n",
    "    - Common models for NLP include Logistic Regression, Naive Bayes, Support Vector Machines (SVM), Random Forest, Gradient Boosting, and neural network-based models like LSTM or BERT.\n",
    "\n",
    "5. Model Training:\n",
    "    - Train the chosen model using the training dataset.\n",
    "\n",
    "6. Model Evaluation:\n",
    "    - Evaluate the model using appropriate metrics like accuracy, F1-score, precision, recall, ROC-AUC, etc.\n",
    "\n",
    "7. Hyperparameter Tuning:\n",
    "    - Optimize the model’s performance by tuning hyperparameters using methods like GridSearchCV or RandomizedSearchCV.\n",
    "  \n",
    "8. Model Validation:\n",
    "    - Validate the model using cross-validation to ensure its robustness and generalizability.\n",
    "\n",
    "9. Model Deployment:\n",
    "    - Save the trained model and vectorizer using joblib or pickle.\n",
    "  \n",
    "10. Inference:\n",
    "    - Load the saved model and vectorizer for making predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e66f3b2-3823-4f5b-b846-9582ea80b0fe",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c847a-8913-49e4-a180-2ec1f9e7a36c",
   "metadata": {},
   "source": [
    "### What to do after text preprocessing for cluster machine learning problems using NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef3700-8d5a-40b1-b4d1-e657a53e4365",
   "metadata": {},
   "source": [
    "After text prprocessing for clustering in NLP tasks, you typically need to follow these steps:\n",
    "1. Text Vectorization:\n",
    "    - Bag of Words (BoW): Convert text data into vectors using methods like Count Vectorizer or TF-IDF Vectorizer.\n",
    "    - Word Embeddings: Use word embeddings such as Word2Vec, GloVe, or FastText.\n",
    "    - Sentence Embeddings: Use models like Sentence-BERT to convert sentences into dense vectors.\n",
    "\n",
    "2. Dimensionality Reduction (if necessary):\n",
    "    - Apply techniques like PCA (Principal Component Analysis) or t-SNE (t-distributed Stochastic Neighbor Embedding) to reduce the dimensionality of the vectorized text data.\n",
    "\n",
    "3. Clustering Algorithm:\n",
    "    - Choose a clustering algorithm suitable for your problem, such as K-means, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), or Hierarchical Clustering.\n",
    "    - Fit the clustering algorithm to your vectorized text data.\n",
    "\n",
    "4. Evaluation:\n",
    "    - Evaluate the clustering results using metrics like Silhouette Score, Davies-Bouldin Index, or by visually inspecting the clusters (e.g., using t-SNE plots).\n",
    "    - If labeled data is available, use clustering accuracy or Adjusted Rand Index for evaluation.\n",
    "  \n",
    "5. Interpretation and Validation:\n",
    "    - Analyze the resulting clusters to understand the common themes or topics.\n",
    "    - Validate the clusters by reviewing sample texts from each cluster to ensure they make sense.\n",
    "\n",
    "6. Iterate and Improve:\n",
    "    - Based on the evaluation, tweak the preprocessing, vectorization, or clustering parameters.\n",
    "    - Iterate through the process to improve cluster quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc997c37-a8bf-40f8-a8a2-08b86d8a5084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
